
% Le type de votre document
\documentclass[a4paper,10pt]{article}

% Quelques packages pour le francais, vous pouvez saisir du texte accentué.
\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{verbatim}
% Des trucs biens pour le pdf.
\usepackage{ae}
\usepackage{aeguill}
\usepackage[bookmarks=true,colorlinks,linkcolor=blue]{hyperref}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage{makeidx}
\usepackage{slashbox}
\usepackage{multirow}
\usepackage{array}
\usepackage[table]{xcolor}


% Pour inclure des graphiques.
\usepackage{graphicx}

\usepackage{tikz}
\usepackage{pgfplots}


% Pour faire déborder les cases d'un tableau sur plusieurs lignes.
\usepackage{multirow}

\usepackage{geometry}

\usepackage[final]{pdfpages}

\usepackage[french]{algorithme}

%\geometry{top=3cm, bottom=3cm, left=2.5cm, right=2.5cm}

\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{exemple}{Exemple}

\begin{document}

%\includepdf[pages=1]{./couverture.pdf}
%\includepdf[pages=1]{./nonPlagiat.pdf}
%
%\section*{\begin{center}Remerciements\end{center}}
%\par Tout d'abord, je tiens à remercier Frédéric LARDEUX et Frédéric SAUBION pour leur écoute, leurs disponibilités et leurs explications et pour la liberté qu'ils m'ont laissé lors de la réalisation de ce projet.
%\par Je remercie également les membres du LERIA\footnote{Laboratoire d'études et de recherche en informatique d'Angers}, pour m'avoir accordé leur confiance en m'attribuant ce stage.
%\par Je remercie l'équipe enseignante du DAEU\footnote{Diplôme d'accès aux études universitaire} d'Angers, et particulièrement Eric SCHRAFSTETTER qui m'a mis le pied à l'étrier pour intégrer la faculté des sciences d'Angers.
%\par Enfin, je remercie ma famille, qui m'a toujours soutenu dans la reprise de mes études, et tout particulièrement Lucie, Marius et Paulin qui partagent mes humeurs et mon quotidien.
%
%
%\newpage
%\null
%\begin{flushright}
%\vspace{\fill}
%\textbf{À Marius et Paulin.}
%\vspace{\fill}
%\end{flushright}


\section*{Introduction}
\subsection*{Sujet du stage}
\par La plupart des bactéries appartenant au genre Xanthomonas sont responsables de pathologies sur
une large gamme de cultures économiquement importantes, induisant notamment des pertes
de rendement et diminuant ainsi la valeur marchande des semences. Quelques graines
contaminées suffisent à générer une source d'inoculation primaire et à occasionner ainsi une
dissémination ultérieure plus large. En particulier, le pathovar phaseoli de Xanthomonas axonopodis
(Xap) qui regroupe toutes les souches identifiées comme pathogènes sur le haricot n'est pas
endémique en Europe mais pour limiter son introduction, il est inscrit sur la liste des agents
pathogènes de quarantaine. Une approche possible pour l'identification des souches bactériennes
consiste à utiliser un répertoire de gènes de virulence. Il s'agit ainsi de trouver la plus petite
combinaison de gènes de virulence spécifiques. Cette combinaison peut ainsi être utilisée pour
concevoir un test d'identification. Des travaux préliminaires[CHHEL et al.2013] montrent que la combinaison des
tests moléculaires ainsi obtenus fournit une technique rapide pour l'identification de toutes les
souches de Xanthomonas pathogènes sur les haricots.
\par Avec les possibilités accrues d'acquisition de données génomiques – par exemple le séquençage à
haut débit – mais également phénotypiques, le problème de la caractérisation de données
biologiques devrait rapidement devenir l'un des verrous essentiel de l'exploitation effective des
grandes bases de données qui sont en cours de constitution, et constituera donc un centre d'intérêt
commun aux biologistes des domaines du végétal ou de la santé. La caractérisation telle que nous
l'entendons permet d'identifier les caractères propres, éventuellement hétérogènes, d'un groupe
d’individus partageant des spécificités fonctionnelles communes (par exemple pathologiques).
\par Du point de vue informatique, ce problème est abordé comme la recherche d'un ensemble de
formules propositionnelles (variables booléennes) permettant de caractériser de manière exacte les
groupes de pathogènes. Les algorithmes mis en jeu reposent sur des explorations arborescentes
(Branch \& Bound) et des algorithmes heuristiques (recherche locale).
\par L'objectif de ce stage est double :
\begin{itemize}
\item D'une part il s'agit de constituer de nouveaux jeux de données, en dialogue avec nos collègues
biologistes. Ceci requiert la définition de formats et l'exploration de base de données pour la collecte
d'informations pertinentes. Ce travail sera effectué en lien étroit avec les laboratoires de l'INRA
Angers.
\item D'autre part, il s'agit également d'améliorer les algorithmes existants et de proposer de nouvelles
approches pour traiter des instances de grande taille. Cette phase s'inspire des algorithmes de
résolution de problèmes combinatoires (SAT-CSP).
\end{itemize}

\subsection*{Plan du mémoire}
Dans la première partie de ce mémoire, nous dressons un état de l'art concernant l'étude du MIN-PCM, ensuite nous décrivons les contributions que nous apportons à l'étude de ce problème: nous définissons la notion d'instance difficile, nous proposons des heuristiques de résolution (exactes ou approchées) et nous fournissons les résultats ainsi obtenus.\textit{[Ici, peut etre la génération d'instance difficile].} Enfin, nous tirons les conclusions de nos travaux et nous discutons les perspectives de recherche envisagées\textit{[à court et à long terme]}.

% La table des matières
\newpage
\setcounter{tocdepth}{5}
\tableofcontents
\newpage

\section{État de l'art}

\subsection{Notations}
Dans ce mémoire, nous utilisons les notations suivantes:
\begin{itemize}
\item $\mathcal{X}$ représente l'ensemble des gènes d'une instance non redondante.
\item $\mathcal{G}$ représente l'ensemble des gènes d'une instance non redondante.
\item $\mathcal{E}$ représente l'ensemble des gènes d'une instance non redondante.
\item $|\mathcal{X}|$ représente la cardinalité de l'ensemble $\mathcal{X}$.
\item $|\mathcal{G}|$ représente la cardinalité l'ensemble $\mathcal{G}$.
\item $|\mathcal{E}|$ représente la cardinalité l'ensemble $\mathcal{E}$.
\end{itemize}




\subsection{Présentation du problème}
La plupart des bactéries appartenant au genre Xanthomonas sont responsables de pathologies sur une large gamme de cultures économiquement importantes,  induisant notamment  des pertes de rendement et diminuant ainsi la valeur marchande des semences. Quelques graines contaminées sont suffisantes pour générer une source d'inoculation primaire et occasionner ainsi une dissémination ultérieure plus large. En particulier, le pathovar\footnote{La notion de pathovar correspond à une subdivision du genre ayant des caractéristiques pathologiques observées communes.} phaseoli de  Xanthomonas Axonopodis (Xap) qui regroupe toutes les souches identifiées comme pathogènes sur le haricot \cite{Vauterin1995}, n'est pas endémique en Europe mais pour limiter son introduction, il est inscrit sur la liste des agents pathogènes de quarantaine.

La taxonomie du genre Xanthomonas n'est pas encore pleinement résolue, et la délimitation de certaines espèces dans ce genre fait encore débat \cite{Schaad2005}~; les approches phylogénétiques ne sont alors pas réellement applicables. Une approche possible pour l'identification des souches bactériennes consiste à utiliser un répertoire de gènes de virulence. Il s'agit ainsi de trouver la plus petite combinaison de gènes de virulence spécifiques au Xap. Cette combinaison peut être utilisée pour concevoir un test PCR multiplex pour l'identification de Xap \cite{Boureau2013,Boureau2012} dont le coût est directement lié au nombre de gènes à tester. Les résultats obtenus
montrent que la combinaison des tests moléculaires ainsi obtenus fournit une technique rapide pour l'identification de toutes les souches de Xanthomonas pathogènes sur les haricots. Nous abordons donc ce problème de la caractérisation de souches dans cet article.

Plus formellement, considérons un ensemble d'entités (les souches bactériennes) regroupées en groupes (les pathovars). Chaque entité est définie par la présence ou l'absence d'un ensemble de caractères (les gènes). Au regard de la représentation binaire qui est utilisée, une entité est considérée comme une interprétation booléenne sur les caractères, qui seront donc les variables booléennes du problème. Ainsi, pour chaque groupe, l'ensemble des entités fournit une table de vérité partielle d'une fonction booléenne vraie pour les interprétations correspondant aux entités du groupe et fausse pour toutes les autres entités des autres groupes. Une telle fonction sera appelée caractérisation d'un groupe. Ces fonctions booléennes seront représentées par des formules propositionnelles construites sur un langage fixé.



Le problème de caractérisation multiple consiste ainsi à trouver un ensemble de
formules booléennes de sorte que chaque formule soit une caractérisation. Le terme
multiple désigne le fait que nous considérons un ensemble de groupes dont les
caractérisations sont dépendantes des entités contenues dans ceux-ci mais aussi
des entités appartenant aux autres groupes.

\begin{figure}[H]{Exemple de problème de caractérisation multiple}
\begin{center}
\begin{tabular}{|c||c|c|c|c|}
\hline
\multirow{2}{*}{Souches}&\multirow{2}{*}{Groupes}&\multicolumn{3}{c|}{Caractères
}\\
&&$a$&$b$&$c$\\
\hline
\hline
$e_1$&\multirow{2}{*}{$g_1$}&\cellcolor{lightgray}0&\cellcolor{lightgray}0&0\\
\cline{1-1} \cline{3-5}
$e_2$&&\cellcolor{lightgray}0& \cellcolor{lightgray}0&1\\
\hline
\hline
$e_3$&$g_2$&1&\cellcolor{lightgray}1&\cellcolor{lightgray}1\\
\hline
\hline
$e_4$&\multirow{2}{*}{$g_3$}&1&\cellcolor{lightgray}1&\cellcolor{lightgray}0\\
\cline{1-1} \cline{3-5}
$e_5$&&0&\cellcolor{lightgray}1&\cellcolor{lightgray}0\\
\hline
\end{tabular}
\end{center}
\end{figure}

Dans la figure  nous avons 5 entités réparties dans 3 groupes et dont
la description se base sur un ensemble de 4 caractères.
Résoudre ce problème revient à caractériser chaque groupe. Il faut donc, pour
chaque groupe, trouver une combinaison de  variables permettant de construire une formule vraie pour toutes les
entités du groupe et fausse pour les autres entités des autres
groupes. Dans l'exemple de la figure \ref{CD}, le groupe 1 est caractérisé par
la négation des variables $a$ et $b$  alors que le groupe 2 est caractérisé par
les variables $b$ et $c$. Les souches du groupe 3 ont toutes en commun la
négation de la variable $c$ tout comme l'entité $e_1$ du groupe 1. Il faut donc
ajouter une autre variable ($b$ par exemple) pour être sûr de caractériser le
groupe.

Dans cet article, après avoir défini formellement le problème de caractérisation
multiple, nous étudions certaines de ses propriétés. Nous décrivons deux approches possibles de résolution, puis nous présentons une preuve sur l'inexistence d'un algorithme FPT ({\em fixed parameter tractable}) pour le problème de caractérisation multiple, sous les hypothèses communément admises. Nous étudions ensuite l'impact d'une transformation du problème vers le problème MIN-ONES afin de le traiter comme un programme linéaire. Enfin, nous proposons des résultats expérimentaux.

\subsubsection{Problème de caractérisation multiple}
Nous présentons dans cette section le problème de caractérisation multiple ainsi que  les travaux connexes.

\subsection{Présentation du problème}

\begin{definition}[Instance du PCM]
Une instance du problème de caractérisation multiple est définie par un n-uplet
$({\cal X}, {\cal E}, {\cal G})$ où $\cal X$ est l'ensemble des
variables propositionnelles, ${\cal E}$ est l'ensemble des entités définies sur
$\cal X$ et ${\cal G} \subseteq 2^{\cal E}$.
\end{definition}

Chaque entité représente une affectation booléenne, ou interprétation, définit ainsi  $e : {\cal X} \to \{0,1\}$, où  $0$ et $1$ sont respectivement les valeurs de vérité fausse et vraie. $e(x)$ correspond donc à la valeur de vérité affectée à $x$ dans l'interprétation $e$. Pour une formule propositionnelle  $\phi$ quelconque sur ${\cal X}$, nous notons $e \models \phi $ le fait que l'interprétation $e \in {\cal E}$ satisfait la formule $\phi$. Une entité $e \in {\cal E}$ sera classiquement représentée par un n-uplet de valeur booléennes (un élément de  $ \{0,1\}^n$).   Ainsi, une instance $({\cal X}, {\cal E}, {\cal G})$  peut être vue comme une matrice booléenne dont les $n$ colonnes correspondent aux variables booléennes de ${\cal X}$ et les $m$ lignes aux entités de ${\cal E}$.  Chaque variable $x_j$ correspond à la colonne $j \in \{1, \ldots, n \} $. Chaque entité $e_i$ correspond à une ligne $i \in \{1, \ldots, m \}$.

Nous pouvons appliquer des prétraitements pour réduire la taille de la matrice en réduisant le nombre de variables réellement utiles et/ou le nombre d'entités. Nous définissons ainsi la notion d'instance non redondante.

\begin{definition}[Instance non redondante]
Une instance est non redondante ssi :
\begin{itemize}
\item Il n'existe pas de colonnes dont toutes valeurs sont identiques :

$\nexists j \in \{1,\ldots, n \},\forall i \in \{1, \ldots, m \} $ tel que $a_{ij}=1$ (resp $a_{ij}=0$);
\item Chaque colonne est unique :

$\nexists j \in \{1, \ldots, n \},\forall k \in \{1, \ldots, n \} \setminus \{j\}, \forall i \in \{1, \ldots, m \} $ tel que $a_{ij}=a_{ik}$;
\item Chaque entité est unique : \\$ \nexists i \in \{1, \ldots, m \},
\forall l \in \{1, \ldots, m \} \setminus \{i\},\forall j \in \{1, \ldots, n \},$ tel que $e_i(x_j)=e_l(x_j)$.
\end{itemize}
\end{definition}

L'application d'un algorithme d'élimination de la redondance s'effectue au pire des cas en ${\mathcal O}({|\cal X}|^2+|{\cal E}|^2)$ et peut conduire à une instance mal-formée dont toutes les colonnes ont été supprimées ou possédant des groupes vides.

Une fois l'instance réduite, nous pouvons préciser la notion de solution d'un problème de caractérisation multiple.

\begin{definition}[Caractérisation d'un groupe]
Pour une instance $({\cal X}, {\cal E}, {\cal G})$, une formule $\phi_g$
caractérise un groupe $g\in {\cal G}$ ssi :
$\forall e \in g, e \models \phi_g$ (acceptation des entités du groupe) et
$\forall g' \in {\cal G} \setminus \{g\},\forall e' \in g',  e' \not \models
\phi_g$ (rejet des entités des autres groupes).
\end{definition}

Par extension, nous notons $g \models \phi_g$ le fait que $\phi_g$ caractérise $g$ selon la définition précédente. $Sol(g)$ représente l'ensemble des solutions d'un groupe $g$. $Sol(g)=\{\phi_g|g \models \phi_g\}$.

\begin{definition}[Solution d'un PCM ]
Pour une instance $({\cal X}, {\cal E}, {\cal G})$, une solution
admissible du PCM  est un $|{\cal G}|$-uplet de formules
$\Phi=(\phi_1,\cdots,\phi_{{\cal G}})$ tel que  $\forall i\in \{1,\ldots,|G|\}, g_i \in
{\cal G}$, $g_i \models \phi_i$.
\end{definition}

Soit ${\cal I} = ({\cal X}, {\cal E}, {\cal G})$, $ SOL({\cal I})$ est
l'ensemble de toutes les solutions multiples pour tous les groupes. $SOL({\cal
I})=Sol(g_1) \times \cdots \times Sol(g_{|{\cal G}|})$, où $\times$ désigne le produit cartésien. Pour un ensemble de
groupes ${\cal G}$ et un $|G|$-uplet de formules $\Phi=(\phi_1,\cdots,\phi_{|{\cal G}|})$, nous notons par extension ${\cal G} \models \Phi$ le fait que
$\forall i\in \{1,\ldots,|G|\}, g_i \in {\cal G}$, $g_i \models \phi_i$.

\begin{definition}[Satisfiabilité d'un PCM]
Une instance $({\cal X}, {\cal E}, {\cal G})$ est satisfiable (resp. insatisfiable) ssi
$\forall g \in {\cal G}, Sol(g) \neq \emptyset$ (resp. $\exists g \in {\cal G},
Sol(g) = \emptyset$).
\end{definition}

Il est évident que toute instance possédant deux interprétations identiques dans deux groupes différents n'est pas satisfiable d'où :

\begin{proposition}
Une instance $({\cal X},{\cal E},{\cal G})$ est satisfiable $\Leftrightarrow
\forall g,g' \in {\cal G}, g\neq g', g
\cap g' =\emptyset$.
\end{proposition}

Une conséquence immédiate est qu'une instance non redondante et bien formée
est satisfiable puisque chaque entité a une interprétation unique.
En pratique, le test de satisfiabilité est effectué implicitement lors de
l'étape de prétraitement et à charge de l'utilisateur de modifier ou supprimer
les entités mis en cause dans l'échec de ce test.

\begin{definition}[Taille d'un n-uplet de formule]
\label{Flength}
Pour un n-uplet $\Phi=(\phi_1,\cdots,\phi_n)$ nous avons $|\Phi| =
|\bigcup_{\phi_i} var(\phi_{i})|$, où $var({\phi})$ retourne l'ensemble des
variables de
$\phi$.
\end{definition}

\begin{definition}[k-PCM (problème de décision)]
Soit une instance ${\cal I}$ une
caractérisation multiple minimale $k$ est un ensemble de formules
$\Phi \in Sol({\cal I})$ vérifiant $|\Phi| \leq k$ avec $ k\in \mathbb{N}^{+}$.
\end{definition}

Le problème de caractérisation multiple minimale pour une taille $k$ ne
correspond pas nécessairement à une solution minimale de
$(\phi_1,\cdots,\phi_n)$ telle que chaque $\phi_i$ est un élément minimal de
$Sol(g_i)$. Nous définissons le problème d'optimisation comme suit.

\begin{definition}[MIN-PCM (problème d'optimisation)]
Pour une instance ${\cal I}$, une
caractérisation optimale multiple minimale est  un ensemble de formules
$\Phi^* \in Sol({\cal I})$ vérifiant $|\Phi^*| \leq |\Phi|$ avec $\forall \Phi
\in Sol({\cal I})$
\end{definition}

Reprise des définitions présente dans l'article [CHHEL et al.2013](voir si cela est possible)
\subsubsection*{Minimisation du problème de caractérisation multiple}
La minimisation du problème de caractérisation multiple consiste à définir le plus petit nombre de gène pouvant caractériser une instance PCM.


\subsection{Complexité}
Le problème SET-COVER appartient à la classe de complexité W[2]-complet. Il a été montré dans [CHHEL et al.2013] qu'une instance PCM pouvait être réduite en temps polynômial en une instance SET-COVER. Il en résulte que PCM appartient à la classe de complexité W[2]-complet \footnote{En admettant l'hypothèse que la WEFT-hiérarchie proposé par [Downey, Fellows, 1995] soit correcte.}. Dès lors, il a été prouvé que le MIN-PCM appartient à la classe de complexité W[2]-difficile. L'impact direct de l'appartenance de MIN-PCM à cette classe de complexité est que l'\textbf{unique} possibilité d'améliorier significativement la résolution complète\footnote{Recherche exacte permettant de prouver l'optimalité d'une solution.} d'une instance est de \textbf{travailler sur des heuristiques de choix de variables}(gènes). 

\subsection{Une résolution basés sur les fonctions booléenes partiellement définis}
Nous proposons plusieurs approches de résolution basées sur les fonctions booléennes
partiellement définies. L'objectif n'est pas de proposer l'algorithme le plus efficace pour le PCM mais plutôt de présenter différentes méthodes de résolution afin de pouvoir par la suite observer leurs comportements pour des instances  aux propriétés variées (section \ref{sec::expe}).

Informellement, nous cherchons à minimiser le nombre de colonnes $k$ en essayant
de garder satisfiable l'instance réduite du PCM. Nous utilisons les fonctions booléennes partiellement définies, {\em partially
defined Boolean formula - pdBf},  \cite{Iba99} qui permettent de proposer un
cadre de résolution intéressant.

Une pdBf est vue comme une fonction booléenne pour laquelle  certaines interprétations ne
sont pas définies.
La classe ${C}^{+}$ (resp. ${C}^{-}$)  désigne l'ensemble des exemples positifs
(resp. négatifs).
À partir de toute fonction booléenne, nous pouvons calculer une formule
caractérisant l'ensemble des interprétations, appelée extension. Il en est de
même pour les pdBf où une DNF (formule en forme normale disjonctive) est une
extension facilement calculable caractérisant la classe $\mathcal{C}^+$.

En ce qui concerne le PCM, nous construisons un ensemble de pdBf emboîtées où
chaque classe ${C}^{-}$ est l'union des classes ${C}^{+}$ des autres groupes.
Nous nous appuyons sur la notion de projection pour calculer de nouvelles
solutions.

\begin{definition}[Projection]
Une projection $\pi$ d'une instance ${\cal I}$ de PCM est la donnée d'un
sous-ensemble ${\cal X'} \subseteq {\cal X}$, définissant implicitement
l'instance ${\cal I'} = ({\cal X'},\{\pi(e) \mid e \in  {\cal E} \},\{ \pi(g)
\mid g \in {\cal G}\})$, où $\pi(e)$ est la restriction de $e$ aux variables de
${\cal X'}$, et $\pi(g)$ est $\{\pi(e)\mid e\in g\}$. On appelle
\emph{dimension} d'une projection $\pi$  le cardinal de ${\cal X'}$.
\end{definition}

La minimisation du nombre de colonnes pour le problème PCM revient à chercher la
projection satisfiable de plus petite dimension associée aux pdBf de chaque
groupe.

La principale difficulté du PCM ne dépend pas de la structure des formules de
l'ensemble solution mais du choix des variables présentes dans celui-ci. La
satisfiabilité d'un ensemble de formules de taille $k$  pour une instance du PCM est équivalente au fait que nous cherchons un sous-ensemble de pdBf consistantes, issues d'une projection satisfiable de dimension $k$ sur les pdBf initiales.

\section{Application : test de diagnostic en biologie végétale}

Nous présentons dans cette section l'utilisation du PCM dans le contexte de la biologie végétale.

\subsection{La caractérisation de pathovars}

Comme mentionné en introduction, la bactérie {\em Xanthomonas}  cause de gros
dégâts sur certaines cultures. Les différentes souches de cette bactérie sont
réparties en groupes et leurs caractéristiques (génotypiques ou phénotypiques)
sont connues. L'objectif est de créer un test de diagnostic permettant de
détecter rapidement à quel groupe appartient une souche donnée.

La caractérisation précise des collections de souches bactériennes est un enjeu
scientifique majeur, car ces bactéries sont responsables d'importantes
pathologies végétales, et donc soumises à des procédures de contrôles officiels
(par exemple, en Europe, la directive 2000/29/CE). Le développement de tests de
diagnostic est donc crucial pour identifier systématiquement les
souches de ces espèces. Dans ce contexte, le problème de caractérisation
correspond à l'identification d'un groupe de souches vis-à-vis d'autres groupes,
basée sur la présence ou l'absence de certains caractères particuliers
\cite{plos}. Une souche peut donc être vue comme un vecteur de valeurs binaires
qui reflète la présence (valeur 1) ou l'absence (valeur 0) de ces caractères. De
plus, les tests de diagnostic étant basés sur des puces à ADN, il est nécessaire
de chercher à minimiser les solutions (le nombre de tests associés pour chaque
identification) afin de réduire le coût prohibitif de ces tests. Notons également que pour chaque caractère le spot de test doit être dupliqué sur la puce, ce qui réduit d'autant plus le nombre de caractères utilisables pour un même kit de diagnostic. Ce problème correspond exactement au MIN-PCM.

\subsubsection{Résolution complète}
\paragraph{Exact-Proj-Car}
Le solveur \emph{Exact-Proj-Car}, proposé dans \cite{sac12}, est basé sur une approche complète consistant à valider ou non la présence d'une caractérisation de taille $k$. Deux types d'exploration sont possibles :
\begin{itemize}
 \item Une exploration en largeur consistant à montrer qu'il n'existe aucune caractérisation valide de taille $k$ avant de tester celles de taille $k+1$. En commençant avec $k= \lceil
log_2(|\cal{G}|)\rceil$ nous garantissons donc de trouver la caractérisation valide optimale. En effet, il est impossible de distinguer plus de $2^k$ classes avec une projection de dimension $k$.
 \item Une exploration en profondeur partant de la caractérisation maximale (toutes les variables) et recherchant une caractérisation valide de taille $k-1$ dès qu'une caractérisation valide de taille $k$ est trouvée.
\end{itemize}

Ces deux approches garantissent toutes les deux de trouver une caractérisation valide optimale mais en cas d'arrêt de la recherche (temps limite atteint, ...), seule l'exploration en profondeur est capable de fournir une caractérisation valide. De plus, en termes de nombre de caractérisations testées, l'exploration en largeur semble la plus coûteuse (hormis pour les solutions de très petite taille). En effet, il est évident que l'exploration en largeur traitera au moins $\binom{n}{k}$ caractérisations ($n$ étant le nombre de variables totales) alors que pour l'exploration en profondeur il est impossible de prévoir ce nombre (au mieux $n-k$ et au pire $\sum_{i=n}^{i=k}\binom{n}{i}$). Remarquons que l'utilisation conjointe de ces deux méthodes permet de borner la caractérisation optimale.

Ces explorations ont toutes les deux des choix de variables à faire. Pour cela nous avons proposé une heuristique de branchement basée sur un classement des variables de manière statique au début de la recherche. L'ordre utilisé est un calcul d'entropie inspirée par la technique proposée dans \cite{DesVer81} qui privilégie les variables permettant de séparer un groupe par rapport aux autres.

\paragraph{Reformulation en programmation linéaire}

Nous présentons maintenant une reformulation du PCM en programmation linéaire. Nous nous intéressons plus
particulièrement à la minimisation du PCM (MIN-PCM) qui consiste à minimiser la taille  $k$ de la solution. Cette reformulation nous permet d'obtenir de nouveaux résultats de complexité pour le PCM.

\subsection{Reformulation}
Nous présentons une modélisation du
MIN-PCM en programmation entière 0/1 (pseudo-booléen). Nous reformulons le MIN-PCM sous forme d'un problème MIN-ONES. De manière similaire au processus de résolution basé sur les pdBfs, nous nous intéressons aux choix des variables de la solution. Le problème MIN-ONES est un problème d'optimisation défini comme suit.

\begin{definition}[MIN-ONES]
Soit $ \Phi $ une collection de formules booléennes $\phi_i$ (contraintes)
définie sur  $\cal X$. Le problème consiste à trouver une affectation booléenne  sur $\cal X$ telle que
chaque contrainte soit satisfaite tout en minimisant le nombre de variable vraie dans cette affectation.
\end{definition}

La réduction vers MIN-ONES reprend l' idée exposée de la proposition \ref{proposition:equiv}. Nous construisons une formule booléenne avec chaque paire d'entités de groupes différents.

Soit $({\cal X}, {\cal E}, {\cal G})$ une instance du PCM.
Pour toute paire d'entités $\{e_i,e_j\}\in {\cal E}^2$ telle que $e\in g,e'\in
g',g\neq g'$, nous construisons une formule $\phi$ de la manière suivante :\\
$\phi=\bigvee_{x_k \in \{x|x \in {\cal X}, e(x) \neq e'(x)\}} x_k $.

Nous observons que $\phi$ est une clause composée uniquement de littéraux positifs. L'ensemble $\Phi$ de ces formules permet de modéliser entièrement le PCM de manière à le résoudre par programmation entière 0/1.

\begin{center}
\[\begin{array}{l}
Domaine : y_i \in \{0,1\}\\
min : \sum_i y_i\\
s.t.\\
  y_{1}+\cdots+y_{n} \geq 1, \forall(x_{1}\vee\cdots\vee x_{n}) \in
\Phi\\
\end{array}\]
\end{center}

Nous constatons que la transformation d'une instance est bornée par $|{\cal E}|^2$
\begin{exemple}[Transformation vers MIN-ONES en programmation linéaire]
Considérons l'instance :

\begin{center}
\begin{tabular}{|c|c||c|c|c|c|}
\hline
\multirow{2}{*}{Entités}&\multirow{2}{*}{Groupes}&\multicolumn{4}{c|}{Caractères}
\\
&&x1&x2&x3&x4\\
\hline
\hline
e1&g1&1&1&1&0\\
\hline
e2&g1&1&1&1&1\\
\hline
e3&g2&0&0&1&0\\
\hline
e4&g2&0&1&1&1\\
\hline
e5&g3&1&1&0&0\\
\hline
\end{tabular}

\par La transformation en programmation entière 0/1 est la suivante :

\[\begin{array}{l m{2cm} r}
\multicolumn{3}{l}{min : \sum_{i=1}^{4} y_i}\\
\multicolumn{3}{l}{s.t.} \\
  y_1+y_2\geq 1 &&  c_{\{e1,e3\}}\\
  y_1+y_4\geq 1&&  c_{\{e1,e4\}}\\
  y_3 = 1&& c_{\{e1,e5\}}\\
  y_1+y_2+y_4 \geq 1&&  c_{\{e2,e3\}}\\
  y_1 = 1&& c_{\{e2,e4\}}\\
  y_3 + y_4 \geq 1&&  c_{\{e2,e5\}}\\
  y_1+y_2+y_3\geq 1&& c_{\{e3,e5\}}\\
  y_1+y_3 + y_4 \geq 1&&  c_{\{e4,e5\}}
\end{array}\]

\end{center}
où chaque contrainte ($c_{\{e_i,e_j\}}$) correspond au traitement d'une paire
d'entités.
\end{exemple}

Avec la transformation proposée, nous pouvons constater qu'une même contrainte
ne peut apparaître plusieurs fois quel que soit le nombre de groupes du départ si
l'instance est d'une part non redondante pour les entités et d'autre part
satisfiable. Bien évidement cela impose de construire l'ensemble des contraintes
en imposant un ordre sur les entités.

Dans \cite{creignou2001complexity}, les auteurs établissent que  MIN-ONES en
présence de clauses ne possédant que des littéraux positifs est contenu dans la
classe de complexité APX \cite{PapadimitriouY91}, ce qui nous laisse penser
que PCM est approximable.

\subparagraph{Réduction d'une instance du problème en programmation linéaire}
\label{sec::reduc}

La transformation du PCM afin de le résoudre à l'aide de programmation linéaire
produit un nombre extrêmement important de clauses. Afin de réduire ce nombre, il
est envisageable d'utiliser la détection de subsomptions \cite{sub} comme cela
est pour le problème SAT. Ce mécanisme peut s'appliquer à
la simplification d'une instance en programmation entière 0/1 car la définition
particulière des contraintes sur lesquelles nous travaillons le permet
(clauses ne contenant que des littéraux positifs). Si
$var(c_{\{e_i,e_j\}})$, avec $i \neq j$, désigne l'ensemble des variables de
décisions d'une contrainte  produite par la paire d'entités $\{e_i,e_j\}$, nous
pouvons supprimer toutes contraintes ayant un sous-ensemble de variables
correspondant à l'ensemble de variables d'une contrainte. Plus formellement,
nous supprimons toute contrainte $c_{\{e_i,e_j\}}$ telle que $\forall
c_{\{e_k,e_l\}}$,avec $k \neq l$, nous avons  $var(c_{\{e_k,e_l\}})\subset
var(c_{\{e_i,e_j\}})$. Comme pour la
subsomption, il est facile de remarquer que n'importe quelle affectation
validant la contrainte $c_{\{e_k,e_l\}}$ implique la validité de
$c_{\{e_i,e_j\}}$.

Dans l'exemple précédent, nous calculons une solution $y_1=1$ et $y_3=1$
en remarquant que $c_{\{e2,e4\}}$ {resp $c_{\{e1,e5\}}$}  permet de supprimer
$c_{\{e1,e3\}}$, $c_{\{e1,e4\}}$, $c_{\{e2,e3\}}$, $c_{\{e3,e5\}}$ et
$c_{\{e4,e5\}}$ (resp. $c_{\{e2,e5\}}$).

Toutefois, il faut remarquer qu'une subsomption n'est présente que lorsque les conditions suivantes sont vérifiées :
\begin{itemize}
\item deux entités $e_1$ et $e_2$ d'un même groupe possèdent $x$ variables avec la même valeur~;
\item au moins une entité d'un autre groupe a des valeurs identiques pour les $n-x$ variables non identiques de  $e_1$ ou de $e_2$ ($n$ étant le nombre total de variables).
\end{itemize}

La probabilité de réduire une instance en programmation linéaire est donc assez faible ($\frac{1}{2^{2n}}<p<\frac{1}{2^{n}}$) mais nous verrons dans la partie expérimentale qui suit que pour des instances venant de problèmes réels les conditions de subsomption sont souvent réunies.

MIN-ONES
\subsubsection{Résolution par recherche locale}
Comme nous le verrons dans la section suivante, utiliser une approche complète peut s'avérer inefficace dans le cas de PCM de grande taille. Une alternative est donc de résoudre le problème à l'aide d'un algorithme de recherche locale \cite{Hoos2004}. L'optimalité des résultats n'est plus garantie mais une solution de bonne qualité est généralement trouvée assez rapidement.

Le principe de notre approche appelée \emph{LS-Proj-Car} est de parcourir l'espace des projections valides en utilisant une liste tabou \cite{Glover1997} afin de limiter les risques de cycles durant la recherche. La fonction de voisinage est définie par l'ensemble des projections ayant une variable de plus ou de moins que la projection actuelle. Le passage d'un voisin à un autre se fait à l'aide de deux opérateurs : $ajout\_var$ qui ajoute aléatoirement une variable à la projection et $supprime\_var$ qui supprime la première variable permettant d'atteindre une projection valide. La recherche commence avec la projection de dimension maximale (toutes les variables) et applique $supprime\_var$ à chaque fois que cela est possible. La liste tabou permet d'éviter d'ajouter et de supprimer une variable plusieurs fois de suite.

Afin de permettre une exploration plus large de l'espace de recherche, plusieurs redémarrages de la recherche sont effectués. Pour ne pas reparcourir les mêmes projections, un mécanisme d'apprentissage permet de garantir un début de recherche différent pour chaque relance. Son principe est de donner un poids inversement proportionnel à l'ordre de sélection lors des recherches précédentes, pour chacune des variables, afin de pouvoir initier les relances suivantes en sélectionnant des variables ayant un poids faible. Ainsi, chaque relance oriente la recherche vers des zones de l'espace de recherche non explorées.
\subsection{Résultats expérimentaux}


Nous avons mené des expérimentations sur des instances réelles et des instances générées aléatoirement.

\begin{itemize}
\item Les instances réelles (raphv, raphy, rarep, rch8 et rch10) sont tirées de problèmes de caractérisation provenant de l'API BioMérieux basée sur des propriétés bio-chimiques de l'espèce Ralstonia et sur des expressions de gènes de virulences de l'espèce Xanthomonas.
\item Les instances aléatoires sont générées en deux étapes, pour un nombre de variables et de groupes fixés. Tout d'abord, chaque entité est construite de manière aléatoire. Ensuite, une certaine proportion des
entités est répartie de manière équitable entre tous les groupes et les entités
restantes sont affectées une à une aléatoirement aux groupes. La proportion
d'entités affectées aléatoirement est donnée en pourcentage et est appelée
$bruit$. Une instance aléatoire est notée sous la forme : s$graine$-$bruit$.
\end{itemize}

Le tableau \ref{tab:results}, présente plusieurs informations sur les instances ainsi que sur leur résolution par différents algorithmes sont présentées. Les colonnes "entités", "groupes" et "caractères" donnent les propriétés des instances. La colonne "contraintes" fournit le nombre de contraintes nécessaires pour reformuler chaque instance sous forme de programme linéaire, la colonne $r$ désigne le ratio entre le nombre de contraintes et le nombre d'entités et la colonne réduction indique le nombre de contraintes après réduction ("non" si aucune). Les trois dernières colonnes fournissent les résultats obtenus (nombre de variables dans la caractérisation trouvée) pour la résolution en programmation linéaire en utilisant \emph{cplex}  \footnote{\texttt{http://www.ibm.com/software/integration/optimization/cplex-optimizer/}} sur le problème réduit("PL"), pour l'exécution d'\emph{Exact-Proj-Car} ("EPJ") et enfin pour l'application de l'algorithme de recherche locale \emph{LS-Proj-Car} ("LSPC"). Le temps
autorisé pour chaque exécution est 10 minutes.
Les résultats en gras indiquent que les solutions sont optimales.










Afin d'avoir un moyen de comparaison pour nos contributions, nous reproduisons ici les résultats fourni dans [CHHEL et al, 2013]\footnote{Nous nous sommes servi du code mis à disposition sur \url{http://forge.info.univ-angers.fr/~gh/Idas/Ccd/mcps/}}. Les expérimentations sont faites sur une machine composé d'un processeur Intel Core\up{tm} i7-2620M CPU à 2.70GHz (deux cœurs) avec 4 Go Ram tournant sous Linux 64-bits. L'option de compilation -Ofast est activé pour obtenir notre exécutable.
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
Instances & Entités(SR) & Groupes & Gènes(SR) & PL & EPC(DIFF)(SH) & LSPC(DIFF) \\ 
\hline 
s301-0 & 500 & 30 & 400 & - & 13 & 14 \\ 
\hline 
s326-0 & 500 & 10 & 500 & - & 13 & 14 \\ 
\hline 
s413-30 & 500 & 20 & 600 & - & 13 & \textcolor{blue}{13} (14) \\ 
\hline 
s555-20 & 800 & 20 & 800 & - & 13 & \textcolor{blue}{13} (14) \\ 
\hline 
s625-20 & 500 & 5 & 1000 & - & 13 & \textcolor{blue}{13} (14) \\ 
\hline 
s754-10 & 600 & 10 & 200 & - & 13 & 14 \\ 
\hline 
s882-20 & 600 & 10 & 400 & - & 13 & 14 \\ 
\hline 
s2501-70 & 800 & 10 & 800 & - & \textcolor{blue}{15} (14) & 15 \\ 
\hline 
s31294-50 & 200 & 15 & 1000 & 10 & 10 & 11 \\ 
\hline 
s3836-0 & 1000 & 15 & 1000 & - & 16 & 16 \\ 
\hline 
raphv & 109 (108) & 8 & 155 (68) & \textbf{6} & \textbf{6} & 9 \\ 
\hline 
raphy & 113 (112) & 4 & 155 (70) & \textbf{6} & \textbf{6} & 8 \\ 
\hline 
rarep & 112 & 7 & 155 (72) & \textbf{12} & 95 (59) (\textcolor{blue}{39}) & 14 \\ 
\hline 
rch8 & 132 (56) & 21 & 37 (27) & \textbf{9} & \textbf{9} & 9 \\ 
\hline 
rch10 & 173 (112) & 27 & 98 (86) & \textbf{10} & 27 (15) (\textcolor{blue}{25}) & 15 \\ 
\hline 
\end{tabular} 
\end{center}
Colonne PL: Résultats obtenu par reformulation en programmation linéaire sur le solveur IBM \textit{cplex}\footnote{\url{http:://www.ibm.com/software/integration/optimization/cplex-optimizer}}.Les instances marqués par "-" ne pas pu être chargé en mémoire car elles solicitaient plus de 32 Go de RAM.\\
SR: Obtenu après suppression des redondances.\\
DIFF: Résultats affichés dans l'article étant différents de ceux obtenus sur notre machine.\\
SH: Résultats obtenu sans heuristique avec notre programme sur notre machine.\\


Les différences significatives sur les instances rarep et rch10 sont du au fait que ces résultats ont été obtenu en partant d'une borne supérieur égale aux nombre de gènes divisé par deux. De fait , il apparaît clairement que l'instance rarep n'a pas bénéficié de la suppression de ses redondances\footnote{$72/2<59$}.

Pour les comparaisons à venir sur les résultats qui diffèrent, nous prendrons en compte les résultats en bleu.

Pour l'instance rch10, le résultat sans heuristique étant meilleur que celui avec l'heuristique de EPC \footnote{Ce qui peut s'expliquer par une différence de structure et d'implémentation du code source.} sur notre machine, nous sommes en mesure de penser que le résultat indiqué dans l'article est erroné, donc nous choisissons de conserver le résultat sans heuristique.


Nous constatons que le nombre de contraintes est beaucoup plus important que le
nombre d'entités de départ mais il reste faible par rapport à la borne théorique
exprimée plus haut. Comme nous l'avons mentionné dans la section \ref{sec::reduc}, la réduction pour les instances aléatoires n'a aucun effet car les conditions nécessaires n'ont qu'une très faible probabilité d'être vérifiées. Toutefois, il est intéressant d'observer que pour les instances réelles, le taux de réduction est assez élevé, ce qui est cohérent car les groupes sont constitués d'entités très similaires.
Le prétraitement nécessaire à la reformulation du problème sous forme de programmation linéaire peut nécessiter de quelques secondes pour les plus petites instances à plusieurs minutes pour les plus grosses.

\'Etant limités en mémoire (32Go sur une machine 64 bits), nous n'avons pu
lire (chargé en mémoire) les instances aléatoires (noté par "-") avec le solveur \emph{cplex}. Nous remarquons que pour l'instance s31294, cette instance est lu par \emph{cplex} mais malheureusement nous n'avons pu résoudre le problème de manière optimale également pour cause mémoire insuffisante.
Pour les problèmes réels \emph{cplex}
est  rapide (quelques secondes) et optimal. Il est d'ailleurs bien plus
rapide que le solveur \emph{Exact-Proj-Car} (l'ordre de la minute).
Cependant \emph{Exact-Proj-Car} fournit une borne supérieure en utilisant très peu de
mémoire (environ 20 Mo) pour les instances aléatoires. \emph{LS-Proj-Car} permet de garantir en toute circonstance une borne optimale facilement calculable avec peu de mémoire et avec une exécution de l'ordre de la seconde, mais il n'obtient jamais la solution optimale.

Les méthodes de résolution que nous avons proposées permettent de traiter des problèmes réels de taille conséquente. Des caractérisations dans le domaine de la biologie végétale ont donc été réalisées et ont conduit au dépôt d'un brevet pour le dépistage du pathovar phaseoli de Xanthomonas Axonopodis \cite{Boureau2012}. Ce test vient d'être validé au niveau européen comme un des tests officiels de dépistage de ce pathovar. Nous avons de plus mis à disposition une page web \footnote{\texttt{http://forge.info.univ-angers.fr/$\sim$gh/Idas/Ccd/mcps/}} permettant aux biologistes d'obtenir aisément une caractérisation de données correspondant au PCM.


\subsection{Conclusion}

Dans cet article, nous avons défini formellement le problème de caractérisation multiple (PCM) consistant à trouver une formule booléenne qui caractérise chaque groupe d'entités représentées par un ensemble d'interprétations booléennes. Nous avons proposé deux méthodes exactes et une méthode de recherche locale afin de résoudre le PCM.

Une étude de la complexité de ce problème a permis de montrer qu'il n'existait pas d'algorithme FPT pour le résoudre. Une réduction vers le problème SET-COVER amène à la conclusion que le PCM est W[2]-Complet. Cette complexité permet de savoir que les informations apprises lors de la recherche d'une caractérisation de taille $k$ par une méthode exacte sont difficilement utilisables pour la recherche d'une caractérisation de taille $k-1$ ou $k+1$.

Nous avons aussi proposé un codage du problème en programmation linéaire à l'aide d'une transformation vers le problème MIN-ONES. Ceci nous a permis de comparer différentes méthodes de résolutions (méthode exacte, recherche locale, programmation linéaire) pour des instances aléatoires et réelles du PCM. Les instances aléatoires semblent plus difficiles à résoudre que les instances réelles et un travail futur sera d'étudier la génération d'instances afin de définir ce qui rend une instance difficile (nombre d'entités, nombre de groupes, diversité intra et intergroupe). Pour les instances réelles, de bons résultats ont été obtenus. Ils ont pu être valorisés par le dépôt d'un brevet pour le dépistage du pathovar phaseoli de Xanthomonas Axonopodis ainsi que par une validation au niveau européen comme un des tests officiels de dépistage de ce pathovar.

\section{Contributions}

\subsection{Introduction} 
Cette section présente les démarches de recherche qui ont été effectué durant le stage.\\
Dans un premier temps, nous faisons une proposition qui permet d'identifier si une instance est difficile ou non.\\
Ensuite, nous abordons la résolution du problème MIN-PCM avec deux approches différentes :
\begin{itemize}
\item Une recherche exacte qui à la possibilité de prouver la borne minimum du MIN-PCM sur des instances de taille raisonnable.
\item Une recherche approché qui à la possibilité de trouver des solutions de bonne qualité mais non nécessairement optimal en un temps polynômial sur des instance de grande taille.
\end{itemize} 
\textit{TODO: Enfin, nous générons des instances pseudo-aléatoire de différents degrés de difficultés que nous soumettons à nos algorithmes.}

\subsection{Définition d'une instance difficile}
Prenons deux instances: une réelle (rch10) et une aléatoire (s3836-0), voici leurs caractéristiques:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Instances & Entités & Groupes & Gènes & Résolution PL & Résolution EPC\footnote{[Exact-Proj-Car CHHEL et al]} \\ 
\hline 
s3836-0 & 1000 & 15 & 1000 & - & 16 \\ 
\hline
rch10 & 173 & 27 & 98 & \textbf{10}\footnote{En gras = solution optimal} & 14 \\ 
\hline
\end{tabular} 
\end{center}
\vspace{7mm}

A priori, on peut supposer que l'instance aléatoire est plus difficile à résoudre: elle est bien plus volumineuse que l'instance réelle à tel point qu'elle nécessite plus de 32 Go de RAM pour une résolution en programmation linéaire.

Observons leurs résolutions avec notre algorithme sans heuristique:

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10,s3836-0},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={14},
xmax={40},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_s3836.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Résolution sans heuristique de rch10 et s3836-0 : nombre de comparaisons}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10,s3836-0},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={14},
xmax={40}]
\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_s3836.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Résolution sans heuristique de rch10 et s3836-0 : temps d'éxécution}
\end{figure}

Nous apercevons que l'instance aléatoire est facilement résolu jusqu'à une caractérisation de taille 15. Ce n'est pas le cas de l'instance réelle qui ne peut plus caractériser en un temps raisonnable à partir d'une caractérisation de taille 25. Ce type d'observation étant \textbf{systématique} quelque soit les caractéristiques des instances réelles ou aléatoire comparés, nous pouvons alors affirmer que la taille d'une instance ne suffit pas à elle seule pour définir sa difficulté. Dès lors, nous nous posons les deux questions suivantes:\\

\begin{itemize}
\item \textbf{Qu'est ce qui peut bien être à l'origine de cette différence de résolution entre une instance aléatoire et une instance réelle?}
\item \textbf{Existe il une méthode permettant de définir si une instance est difficile à résoudre ou non ?}\\
\end{itemize}
Afin de répondre à ces questions, nous définissons les notions suivantes:

\begin{definition}
Le \textbf{masque $M$ d'un groupe $g$} correspond à la moyenne des présences/absences des gènes pour chaque entité du groupe.\\
Formellement, soit $M_g$ le masque d'un groupe $g$, $g \in \mathcal{G}$, $M_g[i]$ la valeur du masque $g$ en position $i$, $i \in [1,|\mathcal{X}|]$,
% $|\mathcal{X}|$ étant le nombre de gènes de l'instance après la suppression des redondances,
$$\forall i \in  [1, |\mathcal{X}|], M_g[i]= \frac{\sum_{i=1}^{|\mathcal{G}|}e_i}{|\mathcal{G}|} $$
\end{definition}

\begin{definition}
Le \textbf{ratio $r$ d'un masque $M$} correspond au pourcentage de valeur entière (0/1) présentent dans le masque.\\
Formellement, soit $M_g$ le masque d'un groupe $g$, $r_g(I)$ le ratio du groupe $g$ dans l'image $I$, $g \in \mathcal{G}$,
\begin{center}
$$ r_g(I)=\frac{|{i / M_g[i] \in \{0,1\}}|}{|\mathcal{X}|},\forall i \in [1,|\mathcal{X}|]$$
\end{center}
\end{definition}

\subsubsection*{Exemple :}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
\backslashbox{Entités}{Gènes} & g0 & g1 & g2 & g3 & g4 & g5 & g6 & g7 & g8 & g9 \\ 
\hline 
e1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\ 
\hline 
e2 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 1 \\ 
\hline 
e3 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ 
\hline 
e4 & 1 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\ 
\hline 
e5 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 \\ 
\hline 
\hline
Masque & 1 & 1 & 0 & 0.8 & 0.6 & 1 & 0 & 0.4 & 0 & 0.4 \\
\hline
\end{tabular}
\end{center}
Le ratio $r$ de ce groupe est : \\
$r=6/10$\\
soit  $r=0.6$

\begin{definition}
L'\textbf{image $I$ d'une instance} est une matrice en deux dimensions de taille $|\mathcal{G}|*|\mathcal{X}|$ où chaque ligne correspond au masque de chacun des groupes de l'instance.\\
Formellement, soit $I_g$ la ligne g de la matrice $I$ correspondant à l'image de l'instance $\mathcal{I}$, $M_g$ le masque du groupe $g$ de l'instance $\mathcal{I}$,
$$\forall g \in [1,|\mathcal{G}|], I_g=M_g$$
\end{definition}


\begin{definition}
Le \textbf{taux de similarité globale} $\mathcal{T}_j$ d'un gène $j$ correspond à la moyenne des valeurs de la colonne $j$ sur l'image $I$ d'une instance $\mathcal{I}$.\\
Formellement, soit $I$ l'image d'une instance $\mathcal{I}$, $i$ la $i$\up{ème} ligne de $I$, $j$ la $j$\up{ème} colonne de $I$, $I_{ij}$ est la valeur dans $I$ en ligne $i$ et en colonne $j$, $\mathcal{T}_j(I)$ le taux de similarité globale du gène $j$ dans l'image $I$,
$$ \text{Soit } X=\frac{\sum_{i=1}^{|\mathcal{G}|} I_{ij}}{\mathcal{G}} $$ 
$$\text{Si } X<0.5 \text{ alors } \mathcal{T}_j(I)=(0.5-X)*2 $$
$$\text{sinon }\mathcal{T}_j(I)=(0.5-(1-X))*2$$ 
\end{definition}
Ainsi formuler, $\mathcal{T}_j(I) \in [0,1]$, et, plus le taux de similarité globale d'un gène est élevé, plus sa présence(resp. abscence) dans l'instance est redondante.

\begin{definition}
Le \textbf{coefficient de difficulté $\rho$ d'une instance}, correspond à la moyenne des taux de similarité globaux d'une instance.\\
Formellement, soit $I$ l'image d'une instance $\mathcal{I}$, $j$ la $j$\up{ème} colonne de $I$, $\mathcal{T}_j(I)$ le taux de similarité globale du gène $j$ dans l'image $I$,
$$ \rho=\frac{\sum_{j=1}^{|\mathcal{X}|}\tau_j(I)}{|\mathcal{X}|} $$
\end{definition}

\begin{definition}
Le \textbf{coefficient de difficulté $\sigma$ d'une instance}, correspond à la moyenne des ratios des masques d'une instance.\\
Formellement, soit $I$ l'image d'une instance $\mathcal{I}$, $i$ la $i$\up{ème} ligne de $I$, $r_i(I)$ le ratio du groupe $i$ dans l'image $I$,
$$ \sigma=\frac{\sum_{i=1}^{|\mathcal{G}|}r_i(I)} {|\mathcal{G}|} $$
\end{definition}

Reprenons nos deux instances rch10 et s3836-0 et calculons leurs coefficients de difficultés:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
Instances & $\rho$ & $\sigma$ \\ 
\hline 
s3836-0 & 0 & 0.024322 \\ 
\hline
rch10 & 0.626381 & 0.906115 \\ 
\hline
\end{tabular} 
\end{center}

Nous observons que le coefficient de difficulté $\rho$ semble plus significatif que $\sigma$ pour déterminer la difficulté d'une instance, mais nous ne sommes pas en mesure d'indiquer dans quel proportions. Cependant les travaux de [CHHEL et al, 2013] nous indiquent que seul une heuristique sur le choix des variables est en mesure de pouvoir améliorer un algorithme de recherche exacte. Cela nous conforte dans l'idée que $\rho$ a plus d'influence que $\sigma$ sur la difficulté d'une instance. 

\begin{proposition}
Une instance dont le coefficient de difficulté $\rho$ est proche de 1 est une \textbf{instance difficile} à résoudre.
\end{proposition}

\begin{proposition}
Une instance dont le coefficient de difficulté $\rho$ est proche de 1 et dont le coefficient de difficulté $\sigma$ est proche de 0 est une \textbf{instance très difficile} à résoudre.
\end{proposition}

\begin{proposition}
La taille d'une instance (caractérisé par son nombre de gènes et d'entités mais pas son nombre de groupe) est une information sur la difficulté de sa résolution. On peut passer outre cette difficulté uniquement si l'instance à un coeficient $sigma$ proche de 1, dans le cas contraire, cette difficulté ne peut pas être réduite de façon significative par un algorithme.
\end{proposition}

%Les observations sur notre jeu de 15 instances nous permettent de conclure que $\sigma$ en particulier, et $\rho$ dans une moindre mesure, nous permettent de définir ce qu'est une instance difficile. 

\subsubsection{Observations}

Nous présentons ici les instances avec leurs coefficients de difficultés respectifs:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
Instances & Entités(SR) & Groupes & Gènes(SR)& $\rho$ & $\sigma$ & PL & EPC & LSPC \\ 
\hline 
s301-0 & 500 & 30 & 400 & 0.034 & 0.00008 & - & 13 & 14 \\ 
\hline 
s326-0 & 500 & 10 & 500 & 0.033 & 0 & - & 13 & 14 \\ 
\hline 
s413-30 & 500 & 20 & 600 & 0.035 & 0 & - & 13 & 13 \\ 
\hline 
s555-20 & 800 & 20 & 800 & 0.039 & 0.00016 & - & 13 & 13 \\ 
\hline 
s625-20 & 500 & 5 & 1000 & 0.035 & 0 & - & 13 & 13 \\ 
\hline 
s754-10 & 600 & 10 & 200 & 0.034 & 0 & - & 13 & 14 \\ 
\hline 
s882-20 & 600 & 10 & 400 & 0.032 & 0 & - & 13 & 14 \\ 
\hline 
s2501-70 & 800 & 10 & 800 & 0.033 & 0 & - & 15 & 15 \\ 
\hline 
s31294-50 & 200 & 15 & 1000 & 0.065 & 0.072 & 10 & 10 & 11 \\ 
\hline 
s3836-0 & 1000 & 15 & 1000 & 0.024 & 0 & - & 16 & 16 \\ 
\hline 
raphv & 109 (108) & 8 & 155 (68) & 0.588 & 0.581 & \textbf{6} & \textbf{6} & 9 \\ 
\hline 
raphy & 113 (112) & 4 & 155 (70) & 0.609 & 0.332 & \textbf{6} & \textbf{6} & 8 \\ 
\hline 
rarep & 112 & 7 & 155 (72) & 0.651 & 0.498 & \textbf{12} & 39 & 14 \\ 
\hline 
rch8 & 132 (56) & 21 & 37 (27) & 0.569 & 0.933 & \textbf{9} & \textbf{9} & 9 \\ 
\hline 
rch10 & 173 (112) & 27 & 98 (86) & 0.626 & 0.906 & \textbf{10} & 25 & 15 \\ 
\hline 
\end{tabular} 
\end{center}

Toute les instances aléatoires ont un coefficient $\rho$ proche de 0. Mais elles ont pour difficulté leurs coefficients $\sigma$ qui sont proche de 0 également, cela signifie que nous ne pourrons pas réduire de façon significative leurs temps de résolution. L'instance raphv et raphy ont un ordre difficulté similaire, rch8 a un coefficient $\sigma$ proche de 1 ainsi qu'un coefficient $\rho$ satisfaisant, de plus, elle est de faible taille, c'est l'instance qui semble la plus facile à résoudre. L'instance rch10 à un très bon coefficient $\sigma$, elle doit être plus facile à résoudre que l'instance rarep. Ces deux dernières semble être les instances les plus difficiles à résoudre.Cette série d'observations et de raisonnement est corroboré par les résultats obtenus par [CHHEL et al,2013].

\subsection{Recherche exacte}
\subsubsection{Introduction}
Nous présentons les heuristiques ayant été mise en place pour la résolution d'instance MIN-PCM. Chaque nouvelle heuristique est ajouté à(aux) précédentes. Les comparaisons se font sur l'instance rch10, entre l'ancienne heuristique la plus puissante connu et la nouvelle mise en place. 

Une comparaison entre la meilleure combinaison d'heuristique trouvé et l'heuristique "CCD" proposé par [CHHEL et al,2013] dans \textit{Exact-Proj-Car} est présenté à la fin de la cette section.

\subsubsection{Heuristique de trie sur les gènes }
\paragraph{Trie des gènes par taux de similarité $\mathcal{T}$}
Les outils mis en place pour définir ce qu'est une instance difficile vont maintenant nous permettre d'obtenir de nouvelles heuristique. Dans cette section, nous ordonnons les gènes par ordre croissant sur leur taux de similarité globale $\mathcal{T}$. Dès lors, nous parcourons en priorité les gènes présentant un faible taux de similarité. Ce trie par $\mathcal{T}$ permet de générer un arbre de recherche qui a pour particularité de présenter en priorité les branches ayant le plus de chances de fournir une éventuelle solution. Un tel arbre de recherche nous permet de caractériser bien plus vite une instance. Le coût de calcul de ce trie est insignifiant et est effectué de façon unique avant le lancement de l'algorithme de résolution.

\subparagraph{Résultats}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique,rch10 heuristique de trie par $\mathcal{T}$},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={10},
xmax={40},
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/tau_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique de trie par $\mathcal{T}$ : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique,rch10 heuristique de trie par $\mathcal{T}$},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={10},
xmax={40}]
\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/tau_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique de trie par $\mathcal{T}$ : temps d'éxécution}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique de trie par $\mathcal{T}$},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={15},
xmax={40},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/tau_s3836.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique de trie par $\mathcal{T}$ : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique de trie par $\mathcal{T}$},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={15},
xmax={40}]

\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/tau_s3836.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique de trie par $\mathcal{T}$ : temps d'éxécution}
\end{figure}


Sur l'instance réelle rch10, on constante que le nombre de comparaisons d'entités  et le temps d'éxécution est considérablement diminué par l'heuristique. Cependant, on s'aperçoit que cette heuristique a peu d'influence (voir même une influence négative dans le cas présent) sur la résolution d'instance aléatoire car celles-ci ne disposent pas de variations significative entre les $\mathcal{T}$ des gènes. La possibilité que l'influence soit négative nous permet de conclure que le trie des gènes par $\mathcal{T}$ n'est pas un trie optimal. 


\subsubsection{Heuristique de trie sur les groupes}
\paragraph{Trie des groupes par taux de similarité $\Gamma$}
Lors de la précédente section, nous avons ordonné les gènes d'après leurs $\mathcal{T}$. Ici, nous souhaitons ordonner les entités dans le but de pouvoir effectuer des retour arrière(backtrack) lorsque nous parcourons notre arbre de recherche. Nous définissons alors les concepts de taux de similarité locale et de taux se similarité globale d'un groupe.

\begin{definition}
Le \textbf{taux de similarite locale $\tau(g,g')$ d'un groupe $g$} par rapport à un groupe $g'$ correspond à la moyenne des sommes des moyennes des valeurs du masque de $g$ et $g'$.\\
Formellement, soit $g$ et $g'$ deux groupes d'une instance $\mathcal{I}$, $M_g$ le masque du groupe $g$, $M_{g'}$ le masque du groupe $g'$, $M_g[i]$ la valeur du masque du groupe $g$ en position $i$,
$$ \tau(g,g')= \frac{\sum_{i=1}^{|\mathcal{X}|}(\frac{M_g[i]+M_{g'}[i]}{2})}{|\mathcal{X}|}$$
\end{definition}

\begin{definition}
Le \textbf{taux de similarité globale $\Gamma(g)$ d'un groupe $g$} correspond à la moyenne de ses taux de similarité locaux.\\
Formellement, soit $g$ le groupe d'une instance $\mathcal{I}$,
$$ \Gamma(g)=\frac{\sum_{g'=1}^{|\mathcal{G}|}\tau(g,g') /g \neq g'}{|\mathcal{G}|-1} $$
\end{definition}

Pour caractériser une instance, nous devons comparer chaque entité n'appartenant pas au même groupe entre elles. Notre idée est de placer les entités ayant le plus de chance de ne pas caractériser l'instance en premier car cela nous permettrait d'effectuer une coupure le plus haut possible dans notre arbre de recherche. Pour cela nous trions les groupes par ordre décroissant sur leurs taux de similarité globaux $\Gamma$.



\subparagraph{Résultats}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique, newGamma, newgamma2,dynamique},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={25},
xmax={40},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/newgamma_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/newgamma2_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/gamma_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique de trie par $\Gamma$ : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique ,newgamma,newgamma2,dynamique},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={25},
xmax={40}]

\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/newgamma_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/newgamma2_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/gamma_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique de trie par $\Gamma$ : temps d'éxécution}
\end{figure}



\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique de trie par $\Gamma$, ratio},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={15},
xmax={27},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/gamma_s3836.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/ratio_s3836.dat};

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique de trie par $\Gamma$ : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique de trie par $\Gamma$, ratio},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={15},
xmax={27}]
\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/gamma_s3836.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/ratio_s3836.dat};

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique de trie par $\Gamma$ : temps d'éxécution}
\end{figure}

%Nous constatons que notre heuristique permet de baisser significativement le nombre de comparaisons .
\subsubsection{Heuristique de trie sur les entités}

\subsubsection{Heuristique de retour en arrière (backtracking)}
\paragraph{Heuristique des plus mauvais d'abord (pmda)}
Lorsque nous parcourons une instance pour une caractérisation de taille $k$. On peut affecter un poids sur la paire d'entités comparés deux à deux sur les indices de la combinaison courante.

Formellement, soit $e_a$, $e_z$ deux entités n'appartenant pas au même groupe, $\mathcal{C}$ une combinaison de $\mathcal{C}_{|\mathcal{X}|}^k$, $P$ le poids de la paire $\{e_a,e_z\}$.
$$ P = |\{i / e_a(i)=e_z(i), \forall i \in \mathcal{C}\}| $$

\begin{definition}
Si $P>=k-1$ alors la \textbf{paire d'entités $\{e_a,e_z\}$ est critique} car lors du parcours de la prochaine combinaisons, cette paire à la plus forte probabilité de permettre une coupure dans l'arbre de recherche.
\end{definition}

\subparagraph{Exemple :}
Soit les 2 entités suivante:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline 
Groupe & \backslashbox{Entités}{Gènes} & g1 & g2 & g3 & g4 & g5 & g6 \\ 
\hline 
1 & e1 & 1 & 0 & 1 & 0 & 0 & 0 \\ 
\hline 
20 & e400 & 1 & 0 & 0 & 0 & 0 & 1 \\ 
\hline 
\end{tabular}
\end{center}
Supposons que nous sommes dans le cas d'une caractérisation de taille 3, nous parcourons les combinaisons de $\mathcal{C}_6^3 $. Supposons que le groupe 1 soit de taille 1, si nous sommes à la comparaison entre e1 et e400, cela signifie que nous avons déjà effectué 399 comparaisons d'entités. 

Regardons alors la combinaison courante 123 : nous apercevons que seul g3 permet la caractérisation. Comme plusieurs des combinaisons suivantes ne différeront que d'un élément, cet paire d'entités \{e1,e400\} à la plus forte probabilité d'être similiraire lors de la prochaine combinaison, nous gardons donc en mémoire cet ensemble qui a un poids égale à $k-1$.

Supposons que 123 n'ai pas caractérisé notre instance, nous parcourons alors 124 : nous commençons par parcourir les ensembles critiques obtenus lors du parcours précédent, soit la comparaison entre e1 et e400. Le poids est alors égale à $k$, ce qui signifie que nous pouvons arrêter notre recherche sur cette combinaison (car celle ci ne pourra en aucun cas caractériser l'instance). Cependant nous gardons en mémoire cet ensemble critique. Notons que nous avons fait là l'économie de 399 comparaisons d'entités.

Nous parcourons alors 125 : même constat , de nouveau une économie de 399 comparaisons d'entités.

Nous parcourons alors 126 : aucun effet, mais la paire \{e1,e400\} est toujours considérer comme critique.

Nous parcourons alors 134 : aucun effet, mais la paire  \{e1,e400\} n'est plus considéré comme critique car son poids $<k-1$.

\subparagraph{Résultats}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique,rch10 heuristique pmda , rch10 heuristique pmda maj},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={25},
xmax={40},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_noMaj_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_maj_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique pmda : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 sans heuristique,rch10 heuristique pmda, rch10 heuristique pmda maj},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={25},
xmax={40}]

\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_noMaj_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_maj_rch10.dat};

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de rch10 sans heuristique et avec heuristique pmda : temps d'éxécution}
\end{figure}



\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique pmda, rch10 heuristique pmda maj},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={15},
xmax={27},
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_noMaj_s3836.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_maj_s3836.dat};

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique pmda : nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={s3836-0 sans heuristique,s3836-0 heuristique pmda, rch10 heuristique pmda maj},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Temps d'éxécution en seconde},
xmin={15},
xmax={27}]
\addplot +[mark=none] table[x=k,y=temps]{./resultats/sh_s3836.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_noMaj_s3836.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_maj_s3836.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaison de la résolution de s3836-0 sans heuristique et avec heuristique pmda : temps d'éxécution}
\end{figure}

On constante que le nombre de comparaisons d'entités ainsi que le temps d'éxécution sont considérablement diminué par l'heuristique.

Ce type de résultat ayant été observé systématiquement sur un jeu de 15 instances(aléatoire et réelle), on peut conclure que cette heuristique est efficace.

\paragraph{Heuristique des valeurs tabous}

\subsection{Comparaisons entre heuristique}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 trie par $\mathcal{T}$,rch10 pmda noMaj,rch10 pmda maj,rch10 tabou},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={12},
xmax={35},
ymax={1000000000}
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/tau_rch10.dat};
%\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/gamma_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_noMaj_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/pmda_maj_rch10.dat};
\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/tabou_rch10.dat};
%\addplot +[mark=none] table[x=k,y=nbComp]{./resultats/ratio_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaisons des heuristiques sur rch10: nombre de comparaisons}
\end{figure}


\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
legend entries={rch10 trie par $\mathcal{T}$,rch10 pmda noMaj,rch10 pmda maj,rch10 tabou},
%legend style={at={(0.5,1.03)},anchor=south},legend columns=3
xlabel={Caractérisation de taille k},
ylabel={Nombre de comparaisons d'entités},
xmin={12},
xmax={35},
ymax={100}
%title={Résolution sans heuristique de rch10 et s3836-0 sur les comparaisons}
]
\addplot +[mark=none] table[x=k,y=temps]{./resultats/tau_rch10.dat};
%\addplot +[mark=none] table[x=k,y=temps]{./resultats/gamma_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_noMaj_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/pmda_maj_rch10.dat};
\addplot +[mark=none] table[x=k,y=temps]{./resultats/tabou_rch10.dat};
%\addplot +[mark=none] table[x=k,y=temps]{./resultats/ratio_rch10.dat};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{Comparaisons des heuristiques sur rch10: temps d'exécution}
\end{figure}

\subsection{Résultats}

\subsection{Recherche incomplète}
\subsubsection{Résultats}

\subsection{Générateur d'instance difficile}
\subsubsection{Résultats}

\section{Conclusions} 

\subsection{Difficultés rencontrées}

\subsection{Conclusions générales}

\subsection{Évolutions possibles et perspective de recherche}
\subsubsection{A court terme}
\subsubsection{A long terme}


\listoffigures

\listoftables

\subsection*{\begin{center} Résumé \end{center}}
\addcontentsline{toc}{chapter}{Résumé / Abstract}

\par Ce rapport porte sur la ....
\\\\
\textbf{Mots-clés:} azerty

\subsection*{\huge \begin{center} Abstract \end{center}}

\par This report concerns the ....
\\\\
\textbf{Keywords:} azerty


\end{document}


%\part{État de l'art}
%\chapter{C.S.P. et Q.C.S.P.}
%\par
%C.S.P. signifie \texttt{constraint satisfaction problem}, soit problème de satisfaction de contrainte. Il s'agit en fait d'une formalisation d'un problème sous la forme d'un triplet $\langle X, D, C \rangle$ avec :\\ 
%$X$ : l'ensemble des variables du problème\\
%$D$ : le (les) domaine(s) contraint(s) aux variables de $X$\\ 
%$C$ : l'ensemble des contraintes entre/sur les variables de $X$\\
%
%\par L'apparition de quantificateur universel dans le problème initial peut vite faire exploser ce type de représentation dès que la taille des domaines commence à devenir significative. Alors que, l'universalité est une donnée intrinsèque aux problèmes et que nous savons la gérer au sein même d'une machine de Turing\footnote{En effet, une contrainte sur une variable universelle est respectée si l'ensemble des valeurs du domaine de la variable convient pour la contrainte. Il ne s'agit donc là que d'une énumération classique de toute les valeurs possibles de la variable}. Comme la machine sait gérer ce problème de façon interne, nous nous sommes penchés sur une formalisation plus simple des problèmes pouvant contenir des quantificateurs universelle\footnote{Je rappelle que, bien que cela ne soit pas précisé, le quantificateur existentiel est présent de façon implicite dans les problèmes lorsque ceux-ci sont dépourvus de quantificateur universel}, les Q.C.S.P.\\
% 
%\par
%Q.C.S.P. signifie donc \texttt{quantified constraint satisfaction problem}, soit problème de satisfaction de contraintes quantifiées. La formalisation est la même que pour un C.S.P., à ceci près que les variables sont associées au sigle $\forall$(universelle) ou $\exists$(existentielle)
%
%\chapter{Espace de recherche, paysage}
%\par
%Un espace de recherche est un ensemble contenant(potentiellement, virtuellement) toutes les configurations(combinaisons, individus) possibles des variables d'un problème. Parmi ces configurations figurent donc les solutions du problème et des combinaisons pour lesquelles il reste un certain nombre $N$ de conflits.
%
%\par 
%On peut se faire une idée visuelle d'un tel espace de recherche en le modélisant sur un repère orthonormé avec en abscisse chaque élément de l'espace de recherche et en ordonnée le nombre de conflits $N$ de chaque combinaison. Une telle modélisation est appelée paysage de recherche. Nous pourrions par exemple obtenir:\\
%%\includegraphics{paysage.png}\\
%Source image: \href{http://www.tony-lambert.fr/these/sommaire.html}{Thèse Tony LAMBERT.}\\
%
%\par
%Comme vous pouvez le constater sur l'image ci-dessus, un paysage de recherche peut être constitué de vallées et de collines, plus ou moins hautes et profondes. Nous appelons les creux de vallées les plus profondes les minimums globaux qui correspondent à des solutions du problème et le creux des autres vallées des minimums locaux. Nous reviendrons par la suite sur la signification de ces termes.
%
%\chapter{Solveurs de contraintes}
%\par Un solveur de contrainte est un logiciel qui a pour but de trouver une solution à un C.S.P.. Plusieurs type de recherches sont possibles pour essayer d'obtenir cette solution. Ce sont ces différents type de recherche qui sont abordés dans ce chapitre.
%\section{Recherche systématique}
%\par Une recherche systématique va consister à évaluer chaque élément d'un espace de recherche jusqu'à trouver une solution. Ce type de démarche nous amène à deux types de résultats:
%\begin{itemize}
%\item Soit le problème n'a pas de solution.
%\item Soit il a au moins une solution et nous savons quelle est celle-ci.
%\end{itemize}
%Nous pouvons éventuellement obtenir toutes les solutions afin de les comparer et en choisir une optimale.
% 
%\par Même si il existe différentes méthodes\footnote{Branch \& Bound , Forward Checking , Full Look Ahead, ...}, qui permettent de ne pas avoir besoin de parcourir l'intégralité des éléments de l'espace de recherche, afin de résoudre au plus vite le problème, la complexité, au pire, est
%toujours \textbf{exponentielle}. Pour certains type de problème, nous savons donc à priori que cette méthode s’avérera trop coûteuse en temps. Pour cette raison, des 
%méthodes de recherches incomplètes ont été créées.
%
%\section{Recherche incomplète}
%Une recherche incomplète consiste à explorer certains "endroits" d'un espace de recherche. Contrairement aux recherches systématiques, ce type de démarche ne nous apporte donc pas la preuve de la non existentialité  d'une solution. Cependant le "champs" de combinaisons étudié étant restreint, nous pouvons faire en sorte de mettre en place des algorithmes de complexité \textbf{polynomiale}, qui ont pour but de trouver les meilleurs configurations possibles\footnote{Et une solution, si possible.}. Deux grandes stratégies de recherche se distinguent dans ce domaine: les recherches stochastiques et les recherches locales\footnote{Et ces deux stratégies peuvent se combiner l'une à l'autre.}.
%
%\subsection{Recherche stochastique}
%Dans le cas des recherches stochastiques, la ``promenade'' dans le paysage de recherche se fait de manière aléatoire. De fait, la découverte d'une bonne combinaison
% est totalement dépendante du hasard mais les individus obtenus seront très diversifier\footnote{La diversification a pour but de se déplacer dans des zones variées de l'espace de recherche.}.
% 
%\subsection{Recherche locale}
%Dans le cas des recherches locales, nous choisissons des heuristiques(pour les cas particuliers) ou des méta-heuristiques(pour les cas généraux) qui nous permettent de nous ``promener'' dans le paysage du problème afin de trouver les vallées les plus profondes. C'est à dire les combinaisons dont le nombre de conflits $N$ est proche de $0$. Ce type de recherche, indispensable pour les problèmes grande taille nous permet d'obtenir des configurations intéressantes, qui elles même peuvent être combinées ensemble\footnote{Par le biais d'un algorithme génétique par exemple.} afin d'obtenir peut-être une solution. On parle alors d'\textbf{hybridation} de méta-heuristiques.\\
%
%\section{Le piège des minimums locaux}
%L'enjeu le plus important lors d'une recherche locale est donc de ne pas rester bloqué dans un minimum local. Pour cela, on fait souvent appel à l'aléatoire afin de diversifier nos combinaisons et ainsi faire des ``sauts'' dans le paysage qui pourrait nous amener sur le flanc d'une vallée nous amenant à un minimum global. Nous pouvons alors, dans une certaine mesure, parler de méthode de recherche locale en ``partie stochastique''\footnote{Méta-heuristique pour l'optimisation difficile [Dréo et al.]}.
%
%\section{Combiner méthodes de recherche complètes et incomplètes}
%Chacune de ces méthodes possède ses avantages et ses inconvénients, l'idée de ce stage consiste à tirer partie des avantages de chacune d'entre elles afin de réduire au maximum le temps de recherche. En pratique, un solveur de contrainte complet devra être aiguillé par une(des) méthode(s) de recherche incomplète qui lui indiquera les ``lieux'' à ne pas visiter, et donc par conséquent, à favoriser les éléments de l'espace de recherche les plus aptes à correspondre à un minimum global.\\
%\par
%Nous aboutissons donc à un nouveau type de recherche qui a pour caractéristique sur des problèmes de très grande taille de:
%
%\begin{itemize}
%\item nous prouver la non existence d'une solution(aux termes d'une exécution qui peut s’avérer très longue)
%\item nous apporter une solution (voir obtenir un ensemble de solutions) dans un temps raisonnable. 
%\end{itemize}
%
%
%\chapter{Méta-heuristiques}
%\section{Introduction}
%Les méta-heuristiques sont des méthodes de recherche généraliste, en partie stochastique, de résolutions de problèmes. Ce fonctionnement permet de faire face à la très forte cardinalité de certains espaces de recherche. En effet, il n'est pas rare de trouver des problèmes dont le nombre de combinaisons possible dépasse de loin le nombre d'atomes présent dans l'univers(soit environ $10^{80}$). Dans ce cas, même les plus gros super-calculateurs ne font pas le poids avec des méthodes de recherche systématique. On s'aperçoit que l'aléatoire nous est bénéfique pour ce type de problème, mais il ne se suffit pas à lui même. 

%\begin{algorithme}
%\Function{rechercheLocale}{$tailleClique$:\textbf{entier},$vSommets$: \textbf{vecteur<Sommet>}}{entier}
%{
%\Type {$noyeau$}{vecteur<Sommet>}\\
%\Type {$enveloppe$}{vecteur<Sommet>}\\
%\Type {$nbIteration$}{entier}\\
%$noyeau \leftarrow vSommet$[1..$tailleClique$]\\
%$enveloppe \leftarrow vSommet$[$tailleClique$..$vSommet$.taille()]\\
%$nbIteration \leftarrow vSommet$.taille()/$param1$\\
%\Repeat
%{
%	\Type {$alea$}{entier}\\
%	$alea \leftarrow rand() \% enveloppe$.taille()/$param2$;\\
%	$noyeau$.ajouter($enveloppe$[$alea$]);\\
%	$enveloppe$.supprimer($enveloppe$[$alea$]);\\
%	\IfThenElse {estUneClique($noyeau$)}
%		{
%			\Type {$cliqueTemp$}{vecteur<Sommet>};\\
%			$cliqueTemp \leftarrow noyeau$\\
%			$tailleClique \leftarrow cliqueTemp$.taille();
%		}
%		{
%			$noyeau$.supprimerDernier();
%		}
%}
%{$nbIteration >=0 $}
%\Return $tailleClique$
%}
%\caption{Recherche locale. $param1$ et $param2$ sont des paramètres pouvant être réglés par l'utilisateur}
%\end{algorithme}
